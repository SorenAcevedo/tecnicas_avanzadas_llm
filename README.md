# RAG Knowledge System: Arquitectura Escalable para Chatbot Empresarial

## üèóÔ∏è Visi√≥n Arquitect√≥nica

Este proyecto implementa un sistema de **Retrieval-Augmented Generation (RAG)** escalable y robusto para la construcci√≥n de chatbots empresariales basados en conocimiento espec√≠fico extra√≠do mediante web scraping. La arquitectura sigue principios de **Clean Architecture** y **Domain-Driven Design** para garantizar mantenibilidad, escalabilidad y testabilidad.

## üìä Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        PRESENTATION LAYER                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Streamlit UI  ‚îÇ  REST API  ‚îÇ  WebSocket  ‚îÇ  CLI Interface      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        APPLICATION LAYER                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Chatbot Service  ‚îÇ  RAG Orchestrator  ‚îÇ  Query Processor      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          DOMAIN LAYER                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Document Models  ‚îÇ  Embedding Models  ‚îÇ  Query Models          ‚îÇ
‚îÇ  RAG Chain Logic  ‚îÇ  Prompt Templates  ‚îÇ  Business Rules        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      INFRASTRUCTURE LAYER                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Scrapy Engine   ‚îÇ  Vector Database   ‚îÇ  LLM Providers         ‚îÇ
‚îÇ  Data Pipeline   ‚îÇ  Caching Layer     ‚îÇ  Monitoring Stack      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üè≠ Componentes Principales

### 1. **Scraping Engine** (Basado en Scrapy)
- **Justificaci√≥n**: Scrapy es el framework m√°s robusto para web scraping a escala empresarial [1]
- **Caracter√≠sticas**:
  - Middleware personalizado para rotaci√≥n de proxies y user agents
  - Pipeline de procesamiento para limpieza autom√°tica de datos
  - Sistema de colas distribuidas con Redis para manejo de requests
  - Manejo inteligente de rate limiting y respeto de robots.txt

### 2. **Data Processing Pipeline** 
- **Chunking Strategies**: Implementaci√≥n de m√∫ltiples estrategias basadas en:
  - **Semantic Chunking**: Utilizando spaCy para segmentaci√≥n sem√°ntica [2]
  - **Recursive Character Text Splitter**: Para documentos largos con overlap configurable
  - **Document-specific chunking**: Adaptado al tipo de contenido (HTML, PDF, etc.)

### 3. **Vector Database Layer**
- **Opciones Soportadas**:
  - **ChromaDB**: Para desarrollo local y prototipado r√°pido
  - **Pinecone**: Para producci√≥n con alta escalabilidad [3]
  - **Weaviate**: Para casos que requieren grafos de conocimiento [4]
- **Justificaci√≥n**: Abstracci√≥n que permite cambiar entre proveedores seg√∫n necesidades

### 4. **RAG Implementation** (LangChain Framework)
- **Retrieval System**:
  - Hybrid search (dense + sparse vectors) para mejor precisi√≥n [5]
  - Re-ranking con Cross-Encoder para mejorar relevancia
  - Query expansion para manejar sin√≥nimos y variaciones
- **Generation System**:
  - Template system robusto para prompt engineering
  - Chain-of-thought prompting para respuestas complejas
  - Soporte para m√∫ltiples LLM providers

### 5. **LLM Abstraction Layer**
- **Providers Soportados**:
  - **Ollama**: Para modelos open-source (Llama 3, Mistral, CodeLlama)
  - **OpenAI**: Para GPT-4, GPT-3.5-turbo
  - **Anthropic**: Para Claude models
  - **Custom endpoints**: Para modelos propietarios
- **Features**:
  - Fallback autom√°tico entre providers
  - Cost optimization y rate limiting
  - Response caching para consultas frecuentes

### 6. **Monitoring & Observability**
- **Logging**: Structured logging con ELK Stack compatibility
- **Metrics**: Prometheus-compatible metrics para latencia, throughput, y accuracy
- **Tracing**: Distributed tracing para debugging del pipeline RAG
- **Health Checks**: Endpoints para monitoreo de todos los componentes

## üîß Tecnolog√≠as y Justificaciones

| Componente | Tecnolog√≠a | Justificaci√≥n |
|------------|------------|---------------|
| Web Scraping | **Scrapy** | Framework m√°s maduro para scraping empresarial, manejo robusto de concurrencia [1] |
| Text Processing | **spaCy + NLTK** | spaCy para NLP moderno, NLTK para tareas espec√≠ficas de limpieza [2] |
| Vector Embeddings | **sentence-transformers** | State-of-the-art para embeddings sem√°nticos multiidioma [6] |
| Vector Database | **ChromaDB/Pinecone** | ChromaDB para desarrollo, Pinecone para escala production [3] |
| RAG Framework | **LangChain** | Ecosistema m√°s completo para aplicaciones LLM empresariales [7] |
| API Framework | **FastAPI** | Performance superior y documentaci√≥n autom√°tica [8] |
| UI Framework | **Streamlit** | Prototipado r√°pido para interfaces de ML/AI [9] |
| Containerization | **Docker + Docker Compose** | Est√°ndar de la industria para deployment reproducible [10] |
| Queue System | **Redis + Celery** | Manejo as√≠ncrono confiable para tareas de procesamiento [11] |

## üìÅ Estructura del Proyecto

```
rag_knowledge_system/
‚îú‚îÄ‚îÄ üìÇ src/                          # C√≥digo fuente principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ scraping/                 # Motor de web scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ spiders/              # Ara√±as de Scrapy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ pipelines/            # Pipelines de procesamiento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ middlewares/          # Middlewares personalizados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ items/                # Definici√≥n de items
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ data_processing/          # Pipeline de procesamiento de datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ chunking/             # Estrategias de segmentaci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ cleaning/             # Limpieza y normalizaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ embeddings/           # Generaci√≥n de embeddings
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ vector_store/             # Abstracci√≥n de base de datos vectorial
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ rag/                      # Sistema RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ retrievers/           # L√≥gica de recuperaci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ chains/               # Cadenas de procesamiento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ prompts/              # Templates de prompts
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ llm/                      # Abstracci√≥n de modelos LLM
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ providers/            # Proveedores (OpenAI, Ollama, etc.)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ models/               # Configuraci√≥n de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ chatbot/                  # Interfaz de chatbot
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ interface/            # UI con Streamlit
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ handlers/             # L√≥gica de manejo de mensajes
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ core/                     # Funcionalidades base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ config/               # Configuraci√≥n centralizada
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ database/             # Abstracciones de DB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÇ logging/              # Sistema de logging
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÇ monitoring/           # M√©tricas y monitoreo
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ utils/                    # Utilidades compartidas
‚îú‚îÄ‚îÄ üìÇ tests/                        # Suite de testing
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ unit/                     # Tests unitarios
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ integration/              # Tests de integraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ e2e/                      # Tests end-to-end
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ data/                     # Datos de prueba
‚îú‚îÄ‚îÄ üìÇ config/                       # Archivos de configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ scrapy/                   # Configuraci√≥n de Scrapy
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ llm/                      # Configuraci√≥n de LLMs
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ vector_db/                # Configuraci√≥n de DB vectorial
‚îú‚îÄ‚îÄ üìÇ scripts/                      # Scripts de automatizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ setup/                    # Scripts de instalaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ data_pipeline/            # Scripts de pipeline de datos
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ deployment/               # Scripts de deployment
‚îú‚îÄ‚îÄ üìÇ data/                         # Almacenamiento de datos
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ raw/                      # Datos crudos del scraping
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ processed/                # Datos procesados
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ embeddings/               # Vectores generados
‚îú‚îÄ‚îÄ üìÇ docs/                         # Documentaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ architecture/             # Documentos de arquitectura
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ api/                      # Documentaci√≥n de API
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ examples/                 # Ejemplos de uso
‚îî‚îÄ‚îÄ üìÇ notebooks/                    # Jupyter notebooks para an√°lisis
```

## üöÄ Pipeline de Datos

```mermaid
graph TB
    A[Web Scraping] --> B[Raw Data Storage]
    B --> C[Data Cleaning & Preprocessing]
    C --> D[Text Chunking]
    D --> E[Embedding Generation]
    E --> F[Vector Database Storage]
    F --> G[RAG Retrieval System]
    G --> H[LLM Processing]
    H --> I[Response Generation]
    I --> J[Chatbot Interface]
```

## üîí Consideraciones de Arquitectura

### **Escalabilidad**
- **Horizontal Scaling**: Cada componente puede escalarse independientemente
- **Load Balancing**: Nginx para distribuci√≥n de carga en m√∫ltiples instancias
- **Caching Strategy**: Redis para cach√© de embeddings y respuestas frecuentes
- **Database Sharding**: Soporte para particionamiento de datos por dominio

### **Reliability**
- **Circuit Breaker Pattern**: Para manejo de fallos en servicios externos
- **Retry Logic**: Backoff exponencial para operaciones que fallan
- **Health Checks**: Monitoreo continuo de todos los servicios
- **Graceful Degradation**: El sistema funciona con funcionalidad reducida si algunos componentes fallan

### **Security**
- **API Rate Limiting**: Protecci√≥n contra abuso de endpoints
- **Input Validation**: Sanitizaci√≥n de todas las entradas del usuario
- **Secrets Management**: Uso de variables de entorno para credenciales
- **HTTPS/TLS**: Cifrado en tr√°nsito para todas las comunicaciones

### **Performance**
- **Asynchronous Processing**: Uso de async/await para operaciones I/O
- **Connection Pooling**: Reutilizaci√≥n eficiente de conexiones de DB
- **Batch Processing**: Agrupaci√≥n de operaciones para mejorar throughput
- **Response Streaming**: Para respuestas de chatbot en tiempo real

## üìã Instalaci√≥n y Configuraci√≥n

### Pre-requisitos
- Python 3.9+ (se recomienda 3.11)
- [uv](https://github.com/astral-sh/uv) - Gestor de paquetes ultrarr√°pido
- Docker y Docker Compose
- Make
- Git

### Instalaci√≥n con uv (Recomendado)

```bash
# 1. Instalar uv (si no est√° instalado)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Clonar el repositorio
git clone https://github.com/your-org/rag-knowledge-system.git
cd rag_knowledge_system

# 3. Configuraci√≥n r√°pida para nuevos desarrolladores
make quick-start

# 4. Configurar variables de entorno
cp .env.example .env
# Editar .env con tus API keys y configuraciones

# 5. Instalaci√≥n completa del entorno de desarrollo
make setup
```

### Comandos Principales

```bash
# Ver todos los comandos disponibles
make help

# Ejecutar la aplicaci√≥n
make run-api          # API FastAPI en http://localhost:8000
make run-streamlit    # Interfaz Streamlit en http://localhost:8501

# Pipeline de datos
make scrape           # Ejecutar web scraping
make process-data     # Procesar datos
make generate-embeddings  # Generar embeddings
make pipeline         # Pipeline completo

# Desarrollo
make format           # Formatear c√≥digo
make lint            # Verificar calidad del c√≥digo
make test            # Ejecutar tests
make check           # Formato + lint + tests

# Docker
make compose-up      # Levantar todos los servicios
make compose-down    # Detener servicios
```

### Instalaci√≥n con Docker (Alternativa)

```bash
# 1. Clonar repositorio
git clone https://github.com/your-org/rag-knowledge-system.git
cd rag_knowledge_system

# 2. Configurar variables de entorno
cp .env.example .env
# Editar .env con tus configuraciones

# 3. Levantar servicios
docker-compose up -d

# 4. Verificar que los servicios est√©n corriendo
docker-compose ps
```

## üß™ Testing y Validaci√≥n

El proyecto incluye una suite completa de testing:
- **Unit Tests**: >90% coverage para l√≥gica de negocio
- **Integration Tests**: Validaci√≥n de componentes integrados
- **E2E Tests**: Flujos completos de usuario
- **Performance Tests**: Benchmarking de latencia y throughput

## üìà M√©tricas y KPIs

- **Response Time**: <2s para consultas simples, <5s para consultas complejas
- **Accuracy**: >85% en respuestas relevantes (evaluado con conjunto de test)
- **Availability**: 99.5% uptime objetivo
- **Throughput**: >100 consultas/minuto por instancia

## üîÑ Roadmap

- [ ] Implementaci√≥n de fine-tuning para modelos espec√≠ficos de dominio
- [ ] Integraci√≥n con GraphRAG para conocimiento estructurado
- [ ] Soporte multimodal (im√°genes, documentos PDF)
- [ ] Integraci√≥n con sistemas de CRM empresariales
- [ ] A/B testing framework para optimizaci√≥n de prompts

## üìö Referencias

[1] Scrapy Documentation. (2024). "Scrapy: An open source web crawling framework for Python." https://scrapy.org/

[2] Honnibal, M., et al. (2020). "spaCy: Industrial-strength Natural Language Processing." https://spacy.io/

[3] Pinecone. (2024). "Vector Database for Machine Learning Applications." https://www.pinecone.io/

[4] Weaviate. (2024). "Open-source vector database." https://weaviate.io/

[5] Karpukhin, V., et al. (2020). "Dense Passage Retrieval for Open-Domain Question Answering." EMNLP 2020.

[6] Reimers, N., & Gurevych, I. (2019). "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks." EMNLP-IJCNLP 2019.

[7] LangChain. (2024). "Framework for developing applications with LLMs." https://langchain.com/

[8] Ram√≠rez, S. (2024). "FastAPI: Modern, fast web framework for building APIs with Python." https://fastapi.tiangolo.com/

[9] Streamlit. (2024). "The fastest way to build and share data apps." https://streamlit.io/

[10] Docker Inc. (2024). "Docker: Accelerated Container Application Development." https://www.docker.com/

[11] Ask Solem. (2024). "Celery: Distributed Task Queue." https://celeryq.dev/

## üë• Contribuci√≥n

Este proyecto sigue las mejores pr√°cticas de desarrollo:
- **Code Review**: Todas las PR requieren revisi√≥n
- **Continuous Integration**: Tests autom√°ticos en cada commit
- **Documentation**: Documentaci√≥n actualizada con cada feature
- **Semantic Versioning**: Versionado sem√°ntico para releases

## üìÑ Licencia

MIT License - Ver archivo LICENSE para m√°s detalles.

---

**Developed with ‚ù§Ô∏è for Enterprise Knowledge Systems**