import streamlit as st
from langchain_ollama import ChatOllama
import time

# --- CONFIGURACIÃ“N DE LA PÃGINA ---
st.set_page_config(
    page_title="ğŸ§© Asistente IA (Ollama + GPT-OSS)",
    page_icon="ğŸ§ ",
    layout="wide",
)

# --- ESTILOS PERSONALIZADOS ---
st.markdown("""
<style>
    body {
        background-color: #0e1117;
        color: #e0e0e0;
    }
    .main {
        background-color: #161a23;
        border-radius: 15px;
        padding: 20px;
    }
    .stButton > button {
        border-radius: 12px;
        background-color: #0078ff;
        color: white;
        font-weight: bold;
    }
    .stButton > button:hover {
        background-color: #005dcc;
    }
    .chat-bubble {
        padding: 10px 15px;
        border-radius: 10px;
        margin: 5px 0;
        display: inline-block;
    }
    .user-bubble {
        background-color: #1f6feb;
        color: white;
        text-align: right;
        margin-left: auto;
    }
    .bot-bubble {
        background-color: #2d333b;
        color: #e6edf3;
        text-align: left;
        margin-right: auto;
    }
    .time-info {
        font-size: 0.8em;
        color: #a0a0a0;
        margin-left: 8px;
    }
</style>
""", unsafe_allow_html=True)

# --- CARGAR CONTEXTO ---
with open("contexto.txt", "r", encoding="utf-8") as f:
    context = f.read()

# --- CONFIGURAR EL MODELO ---
llm = ChatOllama(
    model="qwen3:1.7b",  # puedes cambiar a "qwen3:1.7b"
    validate_model_on_init=True,
    temperature=0.7,
    num_predict=2048  
)

# --- ESTADO DE SESIÃ“N ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- TÃTULO ---
st.title("ğŸ§  Asistente IA con Contexto Local")
st.caption("Interfaz interactiva impulsada por **Ollama + GPT-OSS**")

# --- CAJA DE CHAT ---
with st.container():
    for msg in st.session_state.messages:
        role, content, duration = msg
        bubble_class = "user-bubble" if role == "user" else "bot-bubble"
        st.markdown(f"<div class='chat-bubble {bubble_class}'>{content}</div>", unsafe_allow_html=True)
        if role == "bot" and duration is not None:
            if duration < 60:
                time_text = f"â±ï¸ Tiempo de respuesta: {duration:.2f} segundos"
            else:
                mins = int(duration // 60)
                secs = int(duration % 60)
                time_text = f"â±ï¸ Tiempo de respuesta: {mins} min {secs} segundos"
            st.markdown(f"<span class='time-info'>{time_text}</span>", unsafe_allow_html=True)

# --- ENTRADA DEL USUARIO ---
user_question = st.text_input("ğŸ’¬ Escribe tu pregunta aquÃ­:")

col1, col2 = st.columns([1, 1])
with col1:
    ask = st.button("ğŸš€ Enviar pregunta")
with col2:
    clear = st.button("ğŸ§¹ Limpiar chat")

if clear:
    st.session_state.messages = []
    st.rerun()

if ask and user_question.strip():
    st.session_state.messages.append(("user", user_question, None))
    with st.spinner("Pensando... ğŸ¤”"):
        start = time.time()
        messages = [
            ("system", "Eres un asistente Ãºtil que responde con precisiÃ³n usando el contexto proporcionado."),
            ("human", f"Contexto:\n{context}\n\nPregunta: {user_question}")
        ]
        response = llm.invoke(messages)
        end = time.time()
        duration = end - start
        st.session_state.messages.append(("bot", response.content, duration))
        st.rerun()

# --- PREGUNTAS PREDEFINIDAS ---
st.subheader("ğŸ¯ Preguntas predefinidas")

preset_questions = [
    "Â¿CuÃ¡l es el propÃ³sito principal del documento?",
    "Resume el contenido en tres frases.",
    "Â¿QuÃ© entidades o personas se mencionan mÃ¡s?",
    "Â¿QuÃ© problema intenta resolver el texto?",
    "Â¿QuÃ© soluciones propone?",
    "Â¿CuÃ¡les son los principales desafÃ­os?",
    "Explica los puntos clave en formato de lista.",
    "Dame un resumen tÃ©cnico.",
    "Dame un resumen ejecutivo.",
    "Â¿QuÃ© datos o mÃ©tricas relevantes aparecen?",
    "Â¿QuÃ© recomendaciones se pueden derivar?",
    "Â¿CuÃ¡l es el tono general del texto?",
    "Â¿QuÃ© supuestos se hacen en el documento?",
    "Â¿QuÃ© partes requieren validaciÃ³n adicional?",
    "Dame una cita textual clave.",
    "Â¿Hay alguna contradicciÃ³n o ambigÃ¼edad?",
    "Â¿CÃ³mo se podrÃ­a aplicar este conocimiento?",
    "Dame un resumen para principiantes.",
    "Dame un resumen para expertos.",
    "PropÃ³n una pregunta adicional relevante."
]

colA, colB = st.columns([2, 1])

with colA:
    for i, q in enumerate(preset_questions, start=1):
        if st.button(f"â“ {i}. {q}"):
            st.session_state.messages.append(("user", q, None))
            with st.spinner(f"Analizando pregunta {i}..."):
                start = time.time()
                messages = [
                    ("system", "Eres un asistente Ãºtil que responde con precisiÃ³n usando el contexto proporcionado."),
                    ("human", f"Contexto:\n{context}\n\nPregunta: {q}")
                ]
                response = llm.invoke(messages)
                end = time.time()
                duration = end - start
                st.session_state.messages.append(("bot", response.content, duration))
                st.rerun()

with colB:
    if st.button("â–¶ï¸ Ejecutar todas secuencialmente"):
        st.info("ğŸš€ Ejecutando preguntas automÃ¡ticamente, por favor espera...")
        progress_bar = st.progress(0)
        status_placeholder = st.empty()

        # âœ… Contenedor persistente donde se irÃ¡n acumulando las respuestas
        chat_container = st.container()

        for i, q in enumerate(preset_questions, start=1):
            status_placeholder.markdown(f"ğŸ”¹ **Pregunta {i}/{len(preset_questions)}:** {q}")

            # Agregar la pregunta al historial y mostrarla
            st.session_state.messages.append(("user", q, None))
            with chat_container:
                st.markdown(f"<div class='chat-bubble user-bubble'>{q}</div>", unsafe_allow_html=True)

            # Llamar al modelo
            with st.spinner(f"Procesando pregunta {i}..."):
                start = time.time()
                messages = [
                    ("system", "Eres un asistente Ãºtil que responde con precisiÃ³n usando el contexto proporcionado."),
                    ("human", f"Contexto:\n{context}\n\nPregunta: {q}")
                ]
                response = llm.invoke(messages)
                end = time.time()
                duration = end - start

            # Agregar respuesta y mostrarla debajo
            st.session_state.messages.append(("bot", response.content, duration))
            with chat_container:
                st.markdown(f"<div class='chat-bubble bot-bubble'>{response.content}</div>", unsafe_allow_html=True)

                if duration < 60:
                    time_text = f"â±ï¸ Tiempo de respuesta: {duration:.2f} segundos"
                else:
                    mins = int(duration // 60)
                    secs = int(duration % 60)
                    time_text = f"â±ï¸ Tiempo de respuesta: {mins} min {secs} segundos"

                st.markdown(f"<span class='time-info'>{time_text}</span>", unsafe_allow_html=True)

            # Actualizar progreso
            progress_bar.progress(i / len(preset_questions))
            time.sleep(0.5)

        status_placeholder.empty()
        st.success("ğŸ‰ EjecuciÃ³n de todas las preguntas completada.")
